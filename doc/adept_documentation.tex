% 
% Adept automatic differentiation library for C++: User guide
%
% Type "pdflatex adept_documentation.tex" twice to recreate the PDF
% file (or type "make pdf" in this directory after running the
% configure script one directory above).
%
% Permission is granted to copy, distribute and/or modify this
% document under the terms of the GNU Free Documentation License,
% Version 1.3 or any later version published by the Free Software
% Foundation. This license may be found at
% http://www.gnu.org/copyleft/fdl.html, and in this directory in the
% "COPYING" file. As an exception, no copyright is asserted for the
% code fragments in this document (indicated in the text with a
% light-grey background); these code fragments are in the Public
% Domain and may be copied, modified and distributed without
% restriction.

\documentclass{article}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue]{hyperref}
\usepackage{natbib}
\usepackage{times}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{color}

% Set math in Times Roman
\DeclareSymbolFont{letters}{OML}{ptmcm}{m}{it}
\DeclareSymbolFont{operators}{OT1}{ptmcm}{m}{n}

% Page set up
\setlength{\oddsidemargin}{0cm} %{0.5cm}
\setlength{\evensidemargin}{0cm} %{0.5cm}
\setlength{\topmargin}{-2cm}
\setlength{\textheight}{24cm}
\setlength{\textwidth}{16cm}
\setlength{\marginparsep}{2cm}
\setlength{\marginparwidth}{0cm}
\setlength{\parindent}{1cm}
\setlength{\parskip}{0cm}
\renewcommand{\baselinestretch}{1.1}
\sloppy

% Configure appearance of code listings
\definecolor{light-gray}{gray}{0.92}
\def\codesize{\small}
\lstset{language=C++,
  backgroundcolor=\color{light-gray},
  numbersep=5pt,
  xleftmargin=0cm,
  xrightmargin=0cm,
  basicstyle=\footnotesize\ttfamily,
  emph={adouble,xdouble,Stack,adept},
  emphstyle=\bfseries\color{red}}
\lstset{showstringspaces=false}

% Table-of-contents configuration
\usepackage{tocloft}
\setlength\cftparskip{-2pt}
\setlength\cftbeforesecskip{1pt}
\setlength\cftaftertoctitleskip{2pt}
\renewcommand\cftsecfont{\normalfont}
\renewcommand\cftsecpagefont{\normalfont}
\renewcommand{\cftsecleader}{\cftdotfill{\cftsecdotsep}}
\renewcommand\cftsecdotsep{\cftdot}
\renewcommand\cftsubsecdotsep{\cftdot}

% Page headers
\usepackage{fancyhdr}
\pagestyle{fancy}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\sectionmark}[1]{\markright{\thesection.\ #1}}
\renewcommand{\subsectionmark}[1]{}
\fancyhead[RO,RE]{\thepage}
\fancyfoot[C]{}

% Symbols and macros
\def\x{\ensuremath{{\bf x}}}
\def\y{\ensuremath{{\bf y}}}
\def\H{\ensuremath{{\bf H}}}
\def\T{\ensuremath{^\mathrm{T}}}
\def\Adept{\emph{Adept}}
\def\code#1{{\codesize\texttt{#1}}}
\def\codebf#1{{\codesize\texttt{\textbf{#1}}}}
\def\citem#1{\item[{\codesize\texttt{#1}}]}
\def\codestyle#1{\texttt{#1}}
\def\Offset{size\_t}

% Title material
\title{\Adept\ automatic differentiation library for C++:\\ User guide}
\author{Robin J. Hogan, University of Reading, UK}
\date{\textit{Document version 1.1 (May 2015) applicable to Adept version 1.1}}
\begin{document}
\maketitle

% Copyright and licensing information for this document
\let\thefootnote\relax\footnote{This document is copyright
  \copyright\ Robin J. Hogan 2013--2015.
  Permission is granted to copy, distribute and/or modify this
  document under the terms of the GNU Free Documentation License,
  Version 1.3 or any later version published by the Free Software
  Foundation. This license may be found at
  \url{http://www.gnu.org/copyleft/fdl.html}.  As an exception, no
  copyright is asserted for the code fragments in this document
  (indicated in the text with a light-grey background); these code
  fragments are in the Public Domain and may be copied, modified and
  distributed without restriction.
  \smallskip

  If you have any queries about \Adept\ that are not answered by this
  document or by the information on the \Adept\ web site
  (\url{http://www.met.reading.ac.uk/clouds/adept/}) then please email
  me at
  \href{mailto:r.j.hogan@reading.ac.uk}{\texttt{r.j.hogan@reading.ac.uk}}.
}

\tableofcontents

\section{Introduction}
\Adept\ (Automatic Differentiation using Expression Templates) is a
software library that enables algorithms written in C and C++ to be
automatically differentiated. It uses an operator overloading
approach, so very little code modification is
required. Differentiation can be performed in forward mode (the
``tangent-linear'' computation), reverse mode (the ``adjoint''
computation), or the full Jacobian matrix can be computed. This
behaviour is common to several other libraries, namely ADOL-C
\citep{Griewank+1996}, CppAD \citep{Bell2007} and Sacado
\citep{Gay2005}, but the use of expression templates, an efficient way
to store the differential information and several other optimizations
mean that reverse-mode differentiation tends to be significantly
faster and use less memory. In fact, \Adept\ is also usually only a
little slower than an adjoint code you might write by hand, but
immeasurably faster in terms of user time; adjoint coding is very time
consuming and error-prone. For technical details of how it works,
benchmark results and further discussion of the factors affecting its
speed when applied to a particular code, see \cite{Hogan2014}.

This user guide describes how to apply the \Adept\ software library to
your code, and many of the examples map on to those in the \code{test}
directory of the \Adept\ software package.  Section
\ref{sec:functionality} describes the functionality that the library
provides. Section \ref{sec:installing} outlines how to install it on
your system and how to compile your own code to use it. Section
\ref{sec:preparation} describes how to prepare your code for automatic
differentiation, and section \ref{sec:adjoint} describes how to
perform forward- and reverse-mode automatic differentiation on this
code. Section \ref{sec:jacobian} describes how to compute Jacobian
matrices. Section \ref{sec:realworld} provides a detailed description
of how to interface an algorithm implemented using \Adept\ with a
third-party minimization library.  Section \ref{sec:withwithout}
describes how to call a function both with and without automatic
differentiation from within the same program. Section
\ref{sec:interfacehandcoded} describes how to interface to software
modules that compute their own Jacobians. Section \ref{sec:tips}
provides some tips for getting the best performance from \Adept.
Section \ref{sec:stack} describes the user-oriented member functions
of the \code{Stack} class that contains the differential information
and section \ref{sec:adouble} describes the member functions of the
``active'' double-precision type \code{adouble}. Section
\ref{sec:status} describes the exceptions that can be thrown by some
\Adept\ functions. Section \ref{sec:configuring} describes how to
configure the behaviour of \Adept\ by defining certain preprocessor
variables, and section \ref{sec:faq} answers some frequently asked
questions. Finally, section \ref{sec:license} describes the license
terms.


\section{What functionality does \Adept\ provide?}
\label{sec:functionality}
\Adept\ provides the following functionality:
%
\begin{description}
\item[Full Jacobian matrix] Given the non-linear function $\y=f(\x)$
  relating vector $\y$ to vector $\x$ coded in C or C++, after a
  little code modification \Adept\ can compute the Jacobian matrix
  $\H=\partial\y/\x$, where the element at row $i$ and column $j$ of
  $\H$ is $H_{i,j}=\partial y_i/\partial x_j$. This matrix will be
  computed much more rapidly and accurately than if you simply
  recompute the function multiple times, each time perturbing a
  different element of $\x$ by a small amount. The Jacobian matrix is
  used in the Gauss-Newton and Levenberg-Marquardt minimization
  algorithms.
\item[Reverse-mode differentiation] This is a key component in
  optimization problems where a non-linear function needs to be
  minimized but the state vector $\x$ is too large for it to make
  sense to compute the full Jacobian matrix. Atmospheric data
  assimilation is the canonical example in the field of
  meteorology. Given a nonlinear function $J(\x)$ relating the
  scalar to be minimized $J$ to vector $\x$, \Adept\ will compute the
  vector of adjoints $\partial J/\partial\x$. Moreover, for a
  component of the code that may be expressed as a multi-dimensional
  non-linear function $\y=f(\x)$, \Adept\ can compute $\partial
  J/\partial\x$ if it is provided with the vector of input adjoints
  $\partial J/\partial\y$.  In this case, $\partial J/\partial\x$ is
  equal to the matrix-vector product $\H\T\partial J/\partial\y$, but
  it is computed here without computing the full Jacobian matrix
  $\H$. The vector $\partial J/\partial\x$ may then be used in a
  quasi-Newton minimization scheme \cite[e.g.,][]{Liu+1989}.
\item[Forward-mode differentiation] Given the non-linear function
  $\y=f(\x)$ and a vector of perturbations $\delta\x$, \Adept\ will
  compute the corresponding vector $\delta\y$ arising from a
  linearization of the function $f$. Formally, $\delta\y$ is equal
  to the matrix-vector product $\H\delta\x$, but it is computed here
  without computing the full Jacobian matrix $\H$. Note that
  \Adept\ is designed for the reverse case, so might not be as fast
  or economical in memory in the forward mode as libraries written
  especially for that purpose (although Hogan, 2013, showed that it
  was competitive).
\end{description}%
%
\Adept\ can currently automatically differentiate the standard
mathematical operators \code{+}, \code{-}, \code{*} and \code{/}, as
well as their assignment versions \code{+=}, \code{-=}, \code{*=} and
\code{/=}. It supports the mathematical functions \code{sqrt},
\code{exp}, \code{log}, \code{log10}, \code{sin}, \code{cos},
\code{tan}, \code{asin}, \code{acos}, \code{atan}, \code{sinh},
\code{cosh}, \code{tanh}, \code{abs} and \code{pow}, and since version
1.1 also the functions \code{asinh}, \code{acosh}, \code{atanh},
\code{expm1}, \code{log1p}, \code{cbrt}, \code{erf}, \code{erfc},
\code{exp2} and \code{log2}. The ``active''
variables can take part in comparison operations \code{==}, \code{!=},
\code{>}, \code{<}, \code{>=} and \code{<=}, as well as the diagnostic
functions \code{isfinite}, \code{isinf} and \code{isnan}.

Note that at present \Adept\ is missing some functionality that you may
require:

\begin{itemize}
\item Differentiation is first-order only: it cannot directly compute
  higher-order derivatives such as the Hessian matrix.
\item It has limited support for complex numbers; no support for
  mathematical functions of complex numbers, and expressions involving
  operations (addition, subtraction, multiplication and division) on
  complex numbers are not optimized.
\item All code to be differentiated in a single program must use the
  same precision. By default this is double precision, although the
  library may be recompiled to use single precision or quadruple
  precision (the latter only if supported by your compiler).
\item Your code should operate on variables individually: they can be
  stored in C-style arrays or \code{std::vector} types, but if you use
  containers that allow operations on entire arrays, such as the
  \code{std::valarray} type, then some array-wise functionality (such
  as mathematical functions applied to the whole array and multiplying
  an array of active variables by an ordinary non-active scalar) will
  not work.
\item It can be applied to C and C++ only; \Adept\ could not be
  written in Fortran since the language provides no template
  capability.
\end{itemize}%
%
It is hoped that future versions will remedy these limitations (and
maybe even a future version of Fortran will support templates).

\section{Installing \Adept\ and compiling your code to use it}
\label{sec:installing}
\Adept\ should be compatible with any C++98 compliant compiler,
although most of the testing has been specifically on Linux with the
GNU C++ compiler. The code is built with the help of a
\code{configure} shell script generated by GNU autotools.  If you are
on a non-Unix system (e.g.\ Windows) and cannot use shell scripts, see
section \ref{sec:non-unix}.
\subsection{Unix-like platforms}
\label{sec:unix}
On a Unix-like system, do the following:
\begin{enumerate}
\item Unpack the package (\code{tar xvfz adept-1.1.tar.gz} on Linux)
  and \code{cd} to the directory \code{adept-1.1}.
\item Configure the build using the \code{configure} script (the use
  of a \code{configure} script generated by \code{autoconf} was
  introduced in \Adept\ version 1.1). The most basic method is to
  just run
\begin{lstlisting}
./configure
\end{lstlisting}
More likely you will wish to compile with a higher level of
optimization than the default (which is \code{-O2}), achieved by
setting the environment variable \code{CXXFLAGS}. You may also wish to
specify the root directory of the installation, say to
\code{/opt/local}. These may be done by running instead
\begin{lstlisting}
./configure "CXXFLAGS=-g -O3" --prefix=/opt/local
\end{lstlisting}
The \code{-g} option to \code{CXXFLAGS} ensures debugging information
is stored.  You can see the options available by running
\code{./configure --help}.  You can turn-off OpenMP parallelization in
the computation of Jacobian matrices using \code{--disable-openmp}.
After compiling the library you may optionally compile a test program
that interfaces to the GNU Scientific Library, or a benchmarking
program that compares the performance to other automatic
differentiation tools ADOL-C, CppAD and Sacado.  If these libraries
are in a non-standard directory then further environment variables
need to be specified in the same way as the example above.  For
example, you can specify an additional directory to search for header
files with something like \code{CPPFLAGS=-I/opt/local/include} or an
additional directory to search for static and shared libraries with
something like \code{LDFLAGS=-L/opt/local/lib}.  See also section
\ref{sec:configuring} for ways to make more fundamental changes to the
configuration of \Adept.  The output from the \code{configure} script
provides information on aspects of how \Adept\ and the test programs
will be built.
\item Build \Adept\ by running
\begin{lstlisting}
make
\end{lstlisting}
This will create the static and shared libraries in \code{adept/.libs}.
\item Install the header file \code{include/adept.h} and the static
  and shared libraries by running
\begin{lstlisting}
make install
\end{lstlisting}
If this is to be installed to a system directory, you will need to log
in as the super-user first.
\item Build the example and benchmarking programs by running
\begin{lstlisting}
make check
\end{lstlisting}
Note that this may be done without first installing the
\Adept\ library to a system directory.  This compiles the following
programs in the \code{test} directory:
\begin{itemize}
\item\code{test\_misc}: the trivial example from \cite{Hogan2014};
\item\code{test\_adept}: compares the results of numerical and
  automatic differentiation;
\item\code{test\_with\_without\_ad}: does the same but compiling the
  same source code both with and without automatic differentiation
  (see \code{test/Makefile} for how this is done),
\item\code{test\_radiances}: demonstrates the interfacing of
  \Adept\ with code that provides its own Jacobian;
\item\code{test\_gsl\_interface}: implementation of a simple minimization
  problem using the L-BFGS minimizer in the GSL library;
\item\code{test\_checkpoint}: demonstration of checkpointing, a useful
  technique for large codes;
\item\code{test\_thread\_safe}: demonstration of the use of multiple
  OpenMP threads, each with its own instance of an \Adept\ stack;
\item\code{test\_no\_lib}: demonstrates the use of the
  \code{adept\_source.h} header file that means there is no need to
  link to the \Adept\ library in order to create an executable.
\item\code{test\_stack\_nesting}: demonstrates that Jacobian
  calculations of certain types of algorithm can be accelerated with
  nested stacks.
\end{itemize}
In the \code{benchmark} directory it compiles
\code{autodiff\_benchmark} for comparing the speed of the
differentiation of two advection algorithms using \Adept, ADOL-C,
CppAD and Sacado (or whichever subset of these tools you have on your
system).  It also compiles \code{animate} for visualizing at a
terminal what the algorithms are doing.  Further information on
running these programs can be found in the \code{README} files in the
relevant directories.  Note that despite the implication, ``\code{make
  check}'' does not automatically run any of the programs it makes to
check they function correctly.
\end{enumerate}

To compile source files that use the \Adept\ library, you need to make
sure that \code{adept.h} is in your include path. If this file is
located in a directory that is not in the default include path, add
something like \code{-I/home/fred/include} to the compiler command
line. At the linking stage, add \code{-ladept} to the command line to
tell the linker to look for the \code{libadept.a} static library, or
equivalent shared library. If
this file is in a non-standard location, also add something like
\code{-L/home/fred/lib} before the \code{-ladept} argument to specify
its location. Section \ref{sec:multipleobjects} provdes an example
Makefile for compiling code that uses the \Adept\ library. Read on to
see how you can compile an \Adept\ application \emph{without} needing
to link to a library.

\subsection{Non-Unix platforms, and compiling \Adept\ applications
  without linking to an external library}
\label{sec:non-unix}
Most of the difficulty in maintaining software that can compile on
multiple platforms arises from the different ways of compiling
software libraries, and the need to test on compilers that may be
proprietary.  Unfortunately I don't have the time to maintain versions
of \Adept\ that build specifically on Microsoft Windows or other
non-Unix platforms.  However, \Adept\ is a fairly small library
amounting to only around 3,400 lines of code, of which around 2,300 are in
the \code{adept.h} header file and around 1,100 in the library source
files.  Therefore I have provided a very simple way to build an
\Adept\ application \emph{without} the need to link to a pre-compiled
\Adept\ library. In one of your source files and one only, add this
near the top:
\begin{lstlisting}
#include "adept_source.h"
\end{lstlisting}
Typically you would include this in the source file containing the
\code{main} function.  This header file is simply a concatenation of
the \Adept\ library source files, so when you compile a file that
includes it, you compile in all the functionally of the
\Adept\ library. All other source files in your application should
include the \code{adept.h} header file as normal.  When you link all
your object files together to make an executable, the
\Adept\ functionality will be built in, even though you did not link
to an external \Adept\ library.  A demonstration of this is in the
\code{test/test\_no\_lib.cpp} source file, which needs to be compiled
together with \code{test/algorithm.cpp} to make an executable.
%
It is hoped that this feature will make it easy to use \Adept\ on
non-Unix platforms, although of course this feature works just as well
on Unix-like platforms as well.  

A further point to note is that, under the terms of the license, it is
permitted to simply copy \code{adept.h} and \code{adept\_source.h}
into an include directory in your software package and use it from
there in both binary and source-code releases of your software. This
means that users do not need to install \Adept\ separately before they
use your software.  However, if you do this then remember that your
use of these files must comply with the terms of the Apache License,
Version 2.0; see section \ref{sec:license} for details.

\section{Code preparation}
\label{sec:preparation}
If you have used ADOL-C, CppAD or Sacado then you will already be
familiar with what is involved in applying an operator-overloading
automatic differentiation package to your code. The user interface to
\Adept\ differs from these only in the detail. It is assumed that you
have an algorithm written in C or C++ that you wish to
differentiate. This section deals with the modifications needed to
your code, while section \ref{sec:adjoint} describes the small
additional amount of code you need to write to differentiate it.

In all source files containing code to be differentiated, you need to
include the \code{adept.h} header file and import the \code{adouble}
type from the \code{adept} namespace. Assuming your code uses double
precision, you then search and replace \code{double} with the ``active''
equivalent \code{adouble}, but doing this only for those variables
whose values depend on the independent input variables.  If you wish
to use a different precision, or to enable your code to be easily
recompiled to use different precisions, then you may alternatively use
the generic \code{Real} type from the \code{adept} namespace with its
active equivalent \code{aReal}. Section \ref{sec:configuring}
describes how to configure these types to represent single, double or
quadruple precision, but be aware that accumulation of round-off error
can make the accuracy of derivatives computed using single precision
insufficient for minimization algorithms. For now we consider only
double precision.

Consider the following contrived algorithm from \cite{Hogan2014} that
takes two inputs and returns one output:

\begin{lstlisting}
 double algorithm(const double x[2]) {
   double y = 4.0;
   double s = 2.0*x[0] + 3.0*x[1]*x[1];
   y *= sin(s);
   return y;
 }
\end{lstlisting}

\noindent The modified code would look like this:

\begin{lstlisting}
 #include "adept.h"
 using adept::adouble;

 adouble algorithm(const adouble x[2]) {
   adouble y = 4.0;
   adouble s = 2.0*x[0] + 3.0*x[1]*x[1];
   y *= sin(s);
   return y;
 }
\end{lstlisting}

\noindent Changes like this need to be done in all source files that
form part of an algorithm to be differentiated. 

If you need to access the real number underlying an \code{adouble}
variable \code{a}, for example in order to use it as an argument to
the \code{fprintf} function, then use \code{a.value()} or
\code{adept::value(a)}. Any mathematical operations performed on
this real number will not be differentiated.

You may use \code{adouble} as the template argument of a Standard
Template Library (STL) vector type (i.e.  \code{std::vector\textless
  adouble\textgreater}), or indeed any container where you access
individual elements one by one. For types allowing mathematical
operations on the whole object, such as the STL \code{complex} and
\code{valarray} types, you will find that although you can multiply
one \code{std::complex\textless adouble\textgreater} or
\code{std::valarray\textless adouble\textgreater} object by another,
mathematical functions (\code{exp}, \code{sin} etc.) will not work
when applied to whole objects, and neither will some simple operations
such as multiplying these types by an ordinary (non-active)
\code{double} variable.  Moreover, the performance is not great
because expressions cannot be fully optimized when in these
containers. It is hoped that a future version of \Adept\ will
include its own complex and vector types that overcome these
limitations.

\section{Applying reverse-mode differentiation}
\label{sec:adjoint}

Suppose you wanted to create a version of \code{algorithm} that
returned not only the result but also the gradient of the result with
respect to its inputs, you would do this:

\begin{lstlisting}
 #include "adept.h"
 double algorithm_and_gradient(
                     const double x_val[2], // Input values
                     double dy_dx[2]) {     // Output gradients
   adept::Stack stack,                      // Where the derivative information is stored
   using adept::adouble;                    // Import adouble from adept
   adouble x[2] = {x_val[0], x_val[1]};     // Initialize active input variables
   stack.new_recording();                   // Start recording
   adouble y = algorithm(x);                // Call version overloaded for adouble args
   y.set_gradient(1.0);                     // Defines y as the objective function 
   stack.compute_adjoint();                 // Run the adjoint algorithm
   dy_dx[0] = x[0].get_gradient();          // Store the first gradient
   dy_dx[1] = x[1].get_gradient();          // Store the second gradient
   return y.value();                        // Return the result of the simple computation
 }
\end{lstlisting}
%
The component parts of this function are in a specific order, and if
this order is violated then the code will not run correctly.
%
\subsection{Set-up stack to record derivative information}
\label{sec:stack_setup}
\begin{lstlisting}
 adept::Stack stack;
\end{lstlisting}
The \code{Stack} object is where the differential version of the
algorithm will be stored. When initialized, it makes itself accessible
to subsequent statements via a global variable, but using thread-local
storage to ensure thread safety. It must be initialized before the
first \code{adouble} object is instantiated, because such an
instantiation registers the \code{adouble} object with the currently
active stack. Otherwise the code will crash with a segmentation fault.

In the example here, the \code{Stack} object is local to the scope of
the function. If another \code{Stack} object had been initialized by
the calling function and so was active at the point of entry to the
function, then the local \code{Stack} object would throw an
\code{adept::stack\_already\_active} exception (see Test 3 described
at \code{test/README} in the \Adept\ package if you want to use
multiple \code{Stack} objects in the same program).  A disadvantage of
local \code{Stack} objects is that the memory it uses must be
reallocated each time the function is called.  This can be overcome in
several ways:
\begin{itemize}
\item Declare the \code{Stack} object to be \code{static}, which means
  that it will persist between function calls. This has the
  disadvantage that you won't be able to use other \code{Stack}
  objects in the program without deactivating this one first (see Test
  3 in the \Adept\ package, referred to above, for how to do this).
\item Initialize \code{Stack} in the main body of the program and pass
  a reference to it to the \code{algorithm\_and\_gradient} function,
  so that it does not go out of scope between calls.
\item Put it in a class so that it is accessible to member functions;
  this approach is demonstrated in section \ref{sec:realworld}.
\end{itemize}
%
\subsection{Initialize independent variables and start recording}
\begin{lstlisting}
 adouble x[2] = {x_val[0], x_val[1]};
 stack.new_recording();
\end{lstlisting}
The first line here simply copies the input values to the algorithm
into \code{adouble} variables. These are the \emph{independent
  variables}, but note that there is no obligation for these to be
stored as one array (as in CppAD), and for forward- and reverse-mode
automatic differentiation you do not need to tell \Adept\ explicitly
via a function call which variables are the independent ones. The next
line clears all differential statements from the stack so that it is
ready for a new recording of differential information.

Note that the first line here actually stores two differential
statements, $\delta$\code{x[0]=0} and $\delta$\code{x[1]=0}, which are
immediately cleared by the \code{new\_recording} function call.  To
avoid the small overhead of storing redundant information on the
stack, we could replace the first line with 
\begin{lstlisting}
 x[0].set_value(x_val[0]);
 x[1].set_value(x_val[1]);
\end{lstlisting}
or
\begin{lstlisting}
 adept::set_values(x, 2, x_val);
\end{lstlisting}
which have the effect of setting the values of \code{x} without storing
the equivalent differential statements.

Previous users of \Adept\ version 0.9 should note that since version
1.0, the \code{new\_recording} function replaces the \code{start}
function call, which had to be put \emph{before} the independent
variables were initialized.  The problem with this was that the
independent variables had to be initialized with the \code{set\_value}
or \code{set\_values} functions, otherwise the gradients coming out of
the automatic differentiation would all be zero.  Since it was easy to
forget this, \code{new\_recording} was introduced to allow the
independent variables to be assigned in the normal way using the
assignment operator (\code{=}).  But don't just replace \code{start}
in your version-0.9-compatible code with \code{new\_recording}; the
latter must appear \emph{after} the independent variables have been
initialized.

\subsection{Perform calculations to be differentiated}
\begin{lstlisting}
 adouble y = algorithm(x);
\end{lstlisting}
The algorithm is called, and behind the scenes the equivalent
differential statement for every mathematical statement is stored in the
stack. The result of the forward calculation is stored in \code{y},
known as a dependent variable. This example has one dependent
variable, but any number is allowed, and they could be returned in
another way, e.g. by passing a non-constant array to algorithm that is
filled with the final values when the function returns.
%
\subsection{Perform reverse-mode differentiation}

\begin{lstlisting}
 y.set_gradient(1.0);
 stack.compute_adjoint();
\end{lstlisting}
The first line sets the initial gradient (or adjoint) of \code{y}. In
this example, we want the output gradients to be the derivatives of
\code{y} with respect to each of the independent variables; to achieve
this, the initial gradient of \code{y} must be unity.

More generally, if \code{y} was only an intermediate value in the
computation of objective function $J$, then for the outputs of the
function to be the derivatives of $J$ with respect to each of the
independent variables, we would need to set the gradient of
\code{y} to $\partial J/\partial$\code{y}. In the case of multiple
intermediate values, a separate call to \code{set\_gradient} is needed
for each intermediate value.  If \code{y} was an array of length
\code{n} then the gradient of each element could be set to the values in a \code{double} array \code{y\_ad} using
\begin{lstlisting}
 adept::set_gradients(y, n, y_ad);
\end{lstlisting}

The \code{compute\_adjoint()} member function of stack performs the
adjoint calculation, sweeping in reverse through the differential
statements stored on the stack. Note that this must be preceded by at
least one \code{set\_gradient} or \code{set\_gradients} call, since
the first such call initializes the list of gradients for
\code{compute\_adjoint()} to act on. Otherwise,
\code{compute\_adjoint()} will throw a
\code{gradients\_not\_initialized} exception.

\subsection{Extract the final gradients}

\begin{lstlisting}
 dy_dx[0] = x[0].get_gradient();
 dy_dx[1] = x[1].get_gradient();
\end{lstlisting}
These lines simply extract the gradients of the objective function
with respect to the two independent variables. Alternatively we could
have extracted them simultaneously using
\begin{lstlisting}
 adept::get_gradients(x, 2, dy_dx);
\end{lstlisting}

To do forward-mode differentiation in this example would involve
setting the initial gradients of \code{x} instead of \code{y}, calling
the member function \code{compute\_tangent\_linear()} instead of
\code{compute\_adjoint()}, and extracting the final gradients from
\code{y} instead of \code{x}.

\section{Computing Jacobian matrices}
\label{sec:jacobian}
Until now we have considered a function with two inputs and one
output.  Consider the following more general function whose declaration
is
\begin{lstlisting}
 void algorithm2(int n, const adouble* x, int m, adouble* y);
\end{lstlisting}
where \code{x} points to the \code{n} independent (input) variables
and \code{y} points to the \code{m} dependent (output) variables. The
following function would return the full Jacobian matrix:
%
\begin{lstlisting}
 #include <vector>
 #include "adept.h"
 void algorithm2_jacobian(
                     int n,                 // Number of input values
                     const double* x_val,   // Input values
                     int m,                 // Number of output values
                     double* y_val,         // Output values
                     double* jac) {         // Output Jacobian matrix
   using adept::adouble;                    // Import Stack and adouble from adept
   adept::Stack stack;                      // Where the derivative information is stored
   std::vector<adouble> x(n);               // Vector of active input variables
   adept::set_values(&x[0], n, x_val);      // Initialize adouble inputs
   adept.new_recording();                   // Start recording
   std::vector<adouble> y(m);               // Create vector of active output variables
   algorithm2(n, &x[0], m, &y[0]);          // Run algorithm
   stack.independent(&x[0], n);             // Identify independent variables
   stack.dependent(&y[0], m);               // Identify dependent variables
   stack.jacobian(jac);                     // Compute & store Jacobian in jac
 }
\end{lstlisting}
%
Note that:
\begin{itemize}
\item The \code{independent} member function of stack is used to
  identify the independent variables, i.e.\ the variables that the
  derivatives in the Jacobian matrix will be with respect to. In this
  example there are \code{n} independent variables located together in
  memory and so can be identified all at once. Multiple calls are
  possible to identify further independent variables.  To identify a
  single independent variable, call \code{independent} with just one
  argument, the independent variable (not as a pointer). 
\item The \code{dependent} member function of stack identifies the
  dependent variables, and its usage is identical to
  \code{independent}.
\item The memory provided to store the Jacobian matrix (pointed to by
  \code{jac}) must be a one-dimensional array of size
  \code{m}$\times$\code{n}, where \code{m} is the number of dependent
  variables and \code{n} is the number of independent variables.
\item The resulting matrix is stored in the sense of the index
  representing the dependent variables varying fastest (column-major
  order).
% To get row-major order, call the \code{jacobian} function
%  with a second argument of \code{true} (see section \ref{sec:stack}).
\item Internally, the Jacobian calculation is performed by multiple
  forward or reverse passes, whichever would be faster (dependent on
  the numbers of independent and dependent variables).
\item The use of \code{std::vector<adouble>} rather than \code{new
  adouble[n]} ensures no memory leaks in the case of an exception being
  thrown, since the memory associated with \code{x} and \code{y} will
  be automatically deallocated when they go out of scope.
\end{itemize}%

\section{Real-world usage: interfacing \Adept\ to a minimization library}
\label{sec:realworld}
Suppose we want to find the vector $\x$ that minimizes an objective function
$J(\x)$ that consists of a large algorithm coded using the
\Adept\ library and encapsulated within a C++ class.  In this section
we illustrate how it may be interfaced to a third-party minimization
algorithm with a C-style interface, specifically the free one in the
GNU Scientific Library.  The full working version of this example,
using the N-dimensional Rosenbrock banana function as the function to
be minimized, is ``Test 4'' in the \code{test} directory of the
\Adept\ software package. The interface to the algorithm is as
follows:
%
\begin{lstlisting}
 #include <vector>
 #include "adept.h"
 using adept::adouble;
 class State {
  public:
    // Construct a state with n state variables
    State(int n) { active_x_.resize(n); x_.resize(n); }
    // Minimize the function, returning true if minimization successful, false otherwise
    bool minimize();
    // Get copy of state variables after minimization
    void x(std::vector<double>& x_out) const;
    // For input state variables x, compute the function J(x) and return it
    double calc_function_value(const double* x);
    // For input state variables x, compute function and put its gradient in dJ_dx
    double calc_function_value_and_gradient(const double* x, double* dJ_dx);
    // Return the size of the state vector
    unsigned int nx() const { return active_x_.size(); }
  protected:
    // Active version: the algorithm is contained in the definition of this function
    adouble calc_function_value(const adouble* x);
    // DATA
    adept::Stack stack_;             // Adept stack object
    std::vector<adouble> active_x_;  // Active state variables
 };
\end{lstlisting}
%
The algorithm itself is contained in the definition of
\code{calc\_function\_value(const adouble*)}, which is implemented using
\code{adouble} variables (following the rules in section
\ref{sec:preparation}). However, the public interface to the class
uses only standard \code{double} types, so the use of \Adept\ is
hidden to users of the class.  Of course, a complicated algorithm may
be implemented in terms of multiple classes that do exchange data via
\code{adouble} objects. We will be using a quasi-Newton minimization
algorithm that calls the algorithm many times with trial vectors $\x$,
and for each call may request not only the value of the function, but
also its gradient with respect to $\x$. Thus the public interface
provides \code{calc\_function\_value(const double*)} and
\code{calc\_function\_value\_and\_gradient}, which could be implemented as
follows:
%
\begin{lstlisting}
 double State::calc_function_value(const double* x) {
   for (unsigned int i = 0; i < nx(); ++i) active_x_[i] = x[i];
   stack_.new_recording();
   return value(calc_function_value(&active_x_[0]));
 }

 double State::calc_function_value_and_gradient(const double* x, double* dJ_dx) {
   for (unsigned int i = 0; i < nx(); ++i) active_x_[i] = x[i];
   stack_.new_recording();
   adouble J = calc_function_value(&active_x_[0]);
   J.set_gradient(1.0);
   stack_.compute_adjoint();
   adept::get_gradients(&active_x_[0], nx(), dJ_dx);
   return value(J);
 }
\end{lstlisting}
%
The first function simply copies the \code{double} inputs into an
\code{adouble} vector and runs the version of
\code{calc\_function\_value} for \code{adouble} arguments. Obviously
there is an inefficiency here in that gradients are recorded that are
then not used, and this function would be typically 2.5--3 times
slower than an implementation of the algorithm that did not store
gradients.  Section \ref{sec:withwithout} describes two ways to
overcome this problem.  The second function above implements
reverse-mode automatic differentiation as described in section
\ref{sec:adjoint}.

The \code{minimize} member function could be implemented using GSL as
follows:
%
\begin{lstlisting}
 #include <iostream>
 #include <gsl/gsl_multimin.h>

 bool State::minimize() {
   // Minimizer settings
   const double initial_step_size = 0.01;
   const double line_search_tolerance = 1.0e-4;
   const double converged_gradient_norm = 1.0e-3;
   // Use the "limited-memory BFGS" quasi-Newton minimizer
   const gsl_multimin_fdfminimizer_type* minimizer_type
     = gsl_multimin_fdfminimizer_vector_bfgs2;

   // Declare and populate structure containing function pointers
   gsl_multimin_function_fdf my_function;
   my_function.n = nx();
   my_function.f = my_function_value;
   my_function.df = my_function_gradient;
   my_function.fdf = my_function_value_and_gradient;
   my_function.params = reinterpret_cast<void*>(this);
   
   // Set initial state variables using GSL's vector type
   gsl_vector *x;
   x = gsl_vector_alloc(nx());
   for (unsigned int i = 0; i < nx(); ++i) gsl_vector_set(x, i, 1.0);

   // Configure the minimizer
   gsl_multimin_fdfminimizer* minimizer
     = gsl_multimin_fdfminimizer_alloc(minimizer_type, nx());
   gsl_multimin_fdfminimizer_set(minimizer, &my_function, x,
                                 initial_step_size, line_search_tolerance);
   // Begin loop
   size_t iter = 0;
   int status;
   do {
     ++iter;
     // Perform one iteration
     status = gsl_multimin_fdfminimizer_iterate(minimizer);

     // Quit loop if iteration failed
     if (status != GSL_SUCCESS) break;
    
     // Test for convergence
     status = gsl_multimin_test_gradient(minimizer->gradient, converged_gradient_norm);
   }
   while (status == GSL_CONTINUE && iter < 100);

   // Free memory
   gsl_multimin_fdfminimizer_free(minimizer);
   gsl_vector_free(x);

   // Return true if successfully minimized function, false otherwise
   if (status == GSL_SUCCESS) {
     std::cout << "Minimum found after " << iter << " iterations\n";
     return true;
   }
   else {
     std::cout << "Minimizer failed after " << iter << " iterations: "
               << gsl_strerror(status) << "\n";
     return false;
   }
 }
\end{lstlisting}
%
The GSL interface requires three functions to be defined, each of
which takes a vector of state variables $\x$ as input:
\code{my\_function\_value}, which returns the value of the function;
\code{my\_function\_gradient}, which returns the gradient of the
function with respect to $\x$; and
\code{my\_function\_value\_and\_gradient}, which returns the value and
the gradient of the function. These functions are provided to GSL as
function pointers (see above), but since GSL is a C library, we need
to use the `\code{extern "C"}' specifier in their definition. Thus the
function definitions would be:
%
\begin{lstlisting}
 extern "C" 
 double my_function_value(const gsl_vector* x, void* params) {
   State* state = reinterpret_cast<State*>(params);
   return state->calc_function_value(x->data);
 }

 extern "C"
 void my_function_gradient(const gsl_vector* x, void* params, gsl_vector* gradJ) { 
   State* state = reinterpret_cast<State*>(params);
   state->calc_function_value_and_gradient(x->data, gradJ->data);
 }

 extern "C"
 void my_function_value_and_gradient(const gsl_vector* x, void* params,
                                     double* J, gsl_vector* gradJ) { 
   State* state = reinterpret_cast<State*>(params);
   *J = state->calc_function_value_and_gradient(x->data, gradJ->data);
 }
\end{lstlisting}
%
When the \code{gsl\_multimin\_fdfminimizer\_iterate} function is
called, it chooses a search direction and performs several calls of
these functions to approximately minimize the function along this
search direction. The \code{this} pointer (i.e.\ the pointer to the
\code{State} object), which was provided to the \code{my\_function}
structure in the definition of the \code{minimize} function above, is
provided as the second argument to each of the three functions
above. Unlike in C, in C++ this pointer needs to be cast back to a
pointer to a \code{State} type, hence the use of
\code{reinterpret\_cast}.

That's it! A call to \code{minimize} should successfully minimize well
behaved differentiable multi-dimensional functions.  It should be
straightforward to adapt the above to work with other minimization
libraries.

\section{Calling an algorithm with and without automatic differentiation from the same program}
\label{sec:withwithout}
The \code{calc\_function\_value(const double*)} member function
defined in section \ref{sec:realworld} is sub-optimal in that it
simply calls the \code{calc\_function\_value(const adouble*)} member
function, which not only computes the value of the function, it also
records the derivative information of all the operations involved.
This information is then ignored. This overhead makes the function
typically 2.5--3 times slower than it needs to be, although sometimes
(specifically for loops containing no trancendental functions) the
difference between an algorithm coded in terms of \code{double}s and
the same algorithm coded in terms of \code{adouble}s can exceed a
factor of 10 \citep{Hogan2014}.  The impact on the computational speed
of the entire minimization process depends on how many requests are
made for the function value only as opposed to the gradient of the
function, and can be significant.  We require a way to avoid the
overhead of \Adept\ computing the derivative information for calls to
\code{calc\_function\_value(const double*)}, without having to
maintain two versions of the algorithm, one coded in terms of
\code{double}s and the other in terms of \code{adouble}s. The three
ways to achieve this are now described.
%
\subsection{Function templates}
\label{sec:func_templates}
The simplest approach is to use a function template for those
functions that take active arguments, as demonstrated in the following
example:
%
\begin{lstlisting}
 #include "adept.h"
 class State {
  public:
    ...
    template <typename xdouble>
    xdouble calc_function_value(const xdouble* x);
    ...
 };

 // Example function definition that must be in a header file included
 // by any source file that calls calc_function_value
 template <typename xdouble>
 inline
 xdouble State::calc_function_value(const xdouble* x) {
   xdouble y = 4.0;
   xdouble s = 2.0*x[0] + 3.0*x[1]*x[1];
   y *= sin(s);
   return y;
 }
\end{lstlisting}
%
This takes the example from section \ref{sec:preparation} and replaces
\code{adouble} by the template type \code{xdouble}. Thus,
\code{calc\_function\_value} can be called with either \code{double}
or \code{adouble} arguments, and the compiler will compile inline the
inactive or active version accordingly.  Note that the function
template need not be a member function of a class.  

This technique is good if only a small amount of code needs to be
differentiated, but for large models the use of inlining is likely to
lead to duplication of compiled code leading to large executables and
long compile times.  The following two approaches do not have this
drawback and are suitable for large codes.

\subsection{Pausable recording}
\label{sec:pausable}

The second method involves compiling the entire code with the
\code{ADEPT\_RECORDING\_PAUSABLE} preprocessor variable defined, which
can be done by adding an argument \code{-DADEPT\_RECORDING\_PAUSABLE}
to the compler command line. This modifies the behaviour of
mathematical operations performed on \code{adouble} variables: instead
of performing the operation and then storing the derivative
information, it performs the operation and then only stores the
derivative information if the \Adept\ stack is not in the ``paused''
state. We then use the following member function definition instead of
the one in section \ref{sec:realworld}:
%
\begin{lstlisting}
 double State::calc_function_value(const double* x) {
   stack_.pause_recording();
   for (unsigned int i = 0; i < nx(); ++i) active_x_[i] = x[i];
   double J = value(calc_function_value(&active_x_[0]));
   stack_.continue_recording();
   return J;
 }
\end{lstlisting}
%
By pausing the recording for all operations on \code{adouble} objects,
most of the overhead of storing derivative information is removed. The
extra run-time check to see whether the stack is in the paused state,
which is carried out by mathematical operations involving
\code{adouble} objects, generally adds a small overhead.  However, in
algorithms where most of the number crunching occurs in loops
containing no trancendental functions, even if the stack is in the
paused state, the presence of the check can prevent the compiler from
agressively optimizing the loop.  In that instance the third method
may be preferable.
%
\subsection{Multiple object files per source file}
\label{sec:multipleobjects}
The third method involves compiling each source file containing
functions with \code{adouble} arguments twice.  The first time, the
code is compiled normally to produce an object file containing
compiled functions including automatic differentiation. The second
time, the code is compiled with the
\code{-DADEPT\_NO\_AUTOMATIC\_DIFFERENTIATION} flag on the compiler
command line. This instructs the \code{adept.h} header file to turn
off automatic differentiation by defining the \code{adouble} type to
be an alias of the \code{double} type. This way, a second set of
object files are created containing overloaded versions of the same
functions as the first set but this time without automatic
differentiation. These object files can be compiled together to form
one executable.  In the example presented in section
\ref{sec:realworld}, the \code{calc\_function\_value} function would
be one that would be compiled twice in this way, once to provide the
\code{calc\_function\_value(const adouble*)} version and the other to
provide the \code{calc\_function\_value(const double*)} version. Note
that any functions that do not include \code{adouble} arguments must
be compiled only once, because otherwise the linker will complain
about multiple versions of the same function.

The following shows a Makefile from a hypothetical project that
compiles two source files (\code{algorithm1.cpp} and
\code{algorithm2.cpp}) twice and a third (\code{main.cpp}) once:
%
\begin{lstlisting}[language=make]
 # Specify compiler and flags
 CPP = g++
 CPPFLAGS = -Wall -O3 -g
 # Normal object files to be created
 OBJECTS = algorithm1.o algorithm2.o main.o
 # Object files created with no automatic differentiation
 NO_AD_OBJECTS = algorithm1_noad.o algorithm2_noad.o
 # Program name
 PROGRAM = my_program
 # Include-file location
 INCLUDES = -I/usr/local/include
 # Library location and name, plus the math library
 LIBS = -L/usr/local/lib -lm -ladept

 # Rule to build the program (typing "make" will use this rule)
 $(PROGRAM): $(OBJECTS) $(NO_AD_OBJECTS)
         $(CPP) $(CPPFLAGS) $(OBJECTS) $(NO_AD_OBJECTS) $(LIBS) -o $(PROGRAM)
 # Rule to build a normal object file (used to compile all objects in OBJECTS)
 %.o: %.cpp
         $(CPP) $(CPPFLAGS) $(INCLUDES) -c $<
 # Rule to build a no-automatic-differentiation object (used to compile ones in NO_AD_OBJECTS)
 %_noad.o: %.cpp
         $(CPP) $(CPPFLAGS) $(INCLUDES) -DADEPT_NO_AUTOMATIC_DIFFERENTIATION -c $< -o $@
\end{lstlisting}
%

There is a further modification required with this approach, which
arises because if a header file declares both the \code{double} and
\code{adouble} versions of a function, then when compiled with
\code{-DADEPT\_NO\_AUTOMATIC\_DIFFERENTIATION} it appears to the
compiler that the same function is declared twice, leading to a
compile-time error.  This can be overcome by using the preprocessor to
hide the \code{adouble} version if the code is compiled with this
flag, as follows (using the example from section \ref{sec:realworld}):
%
\begin{lstlisting}
 #include "adept.h"
 class State {
  public:
    ...
    double calc_function_value(const double* x);
  protected:
 #ifndef ADEPT_NO_AUTOMATIC_DIFFERENTIATION
    adouble calc_function_value(const adouble* x);
 #endif
    ...
 };
\end{lstlisting}

A final nuance is that if the code contains an \code{adouble} object
\code{x}, then \code{x.value()} will work fine in the compilation when
\code{x} is indeed of type \code{adouble}, but in the compilation when
it is set to a simple \code{double} variable, the \code{value()}
member function will not be found.  Hence it is better to use
\code{adept::value(x)}, which returns a \code{double} regardless of
the type of \code{x}, and works regardless of whether the code was
compiled with or without the
\code{-DADEPT\_NO\_AUTOMATIC\_DIFFERENTIATION} flag.

\section{Interfacing with software containing hand-coded Jacobians}
\label{sec:interfacehandcoded}
Often a complicated algorithm will include multiple components.
Components of the code written in C or C++ for which the source is
available are straightforward to convert to using \Adept, following
the rules in section \ref{sec:preparation}.  For components written in
Fortran, this is not possible, but if such components have their own
hand-coded Jacobian then it is possible to interface \Adept\ to them.
More generally, in certain situations automatic differentiation is
much slower than hand-coding \cite[see the Lax-Wendroff example
  in][]{Hogan2014} and we may wish to hand-code certain critical
parts.  In general the Jacobian matrix is quite expensive to compute,
so this interfacing strategy makes most sense if the component of the
algorithm has a small number of inputs or a small number of
outputs. A full working version of the following example is given as
``Test 3'' in the \code{test} directory of the \Adept\ package.

Consider the example of a radiative transfer model for simulating
satellite microwave radiances at two wavelengths, $I$ and $J$, which
takes as input the surface temperature $T_s$ and the vertical profile
of atmospheric temperature $T$ from a numerical weather forecast
model. Such a model would be used in a data assimilation system to
assimilate the temperature information from the satellite observations
into the weather forecast model. In addition to returning the
radiances, the model returns the gradient $\partial I/\partial T_s$
and the gradients $\partial I/\partial T_i$ for all height layers $i$
between 1 and $n$, and likewise for radiance $J$. The interface to the
radiative transfer model is the following:
%
\begin{lstlisting}
 void simulate_radiances(int n, // Size of temperature array
                         // Input variables:
                         double surface_temperature, 
                         const double* temperature,
                         // Output variables:
                         double radiance[2],
                         // Output Jacobians:
                         double dradiance_dsurface_temperature[2],
                         double* dradiance_dtemperature);
\end{lstlisting}
%
The calling function needs to allocate \code{2*n} elements for the
temperature Jacobian \code{dradiance\_dtemperature} to be stored, and
the stored Jacobian will be oriented such that the radiance index
varies fastest.

\Adept\ needs to be told how to relate the radiance perturbations
$\delta I$ and $\delta J$, to perturbations in the input
variables, $\delta T_s$ and $\delta T_i$ (for all layers
$i$). Mathematically, we wish the following relationship to be stored
within the \Adept\ stack:
%
\begin{equation}
\delta I = \frac{\partial I}{\partial T_s}\delta
T_s+\sum_{i=1}^n\frac{\partial I}{\partial T_i}\delta T_i.\nonumber
\end{equation}
%
This is achieved with the following wrapper function, which has
\code{adouble} inputs and outputs and therefore can be called from
within other parts of the algorithm that are coded in terms of
\code{adouble} objects:
%
\begin{lstlisting}
 void simulate_radiances_wrapper(int n,
                                 const adouble& surface_temperature,
                                 const adouble* temperature,
                                 adouble radiance[2]) {
   // Create inactive (double) versions of the active (adouble) inputs
   double st = value(surface_temperature);
   std::vector<double> t(n);
   for (int i = 0; i < n; ++i) t[i] = value(temperature[i]);

   // Declare variables to hold the inactive outputs and their Jacobians
   double r[2];
   double dr_dst[2];
   std::vector<double> dr_dt(2*n);

   // Call the non-Adept function
   simulate_radiances(n, st, &t[0], &r[0], dr_dst, &dr_dt[0]);

   // Copy the results into the active variables, but use set_value in order
   // not to write any equivalent differential statement to the Adept stack
   radiance[0].set_value(r[0]);
   radiance[1].set_value(r[1]);

   // Loop over the two radiances and add the differential statements to the Adept stack
   for (int i = 0; i < 2; ++i) {
     // Add the first term on the right-hand-side of Equation 1 in the text
     radiance[i].add_derivative_dependence(surface_temperature, dr_dst[i]);
     // Now append the second term on the right-hand-side of Equation 1. The third argument
     // "n" of the following function says that there are n terms to be summed, and the fourth 
     // argument "2" says to take only every second element of the Jacobian dr_dt, since the 
     // derivatives with respect to the two radiances have been interlaced.  If the fourth 
     // argument is omitted then relevant Jacobian elements will be assumed to be contiguous
     // in memory.
     radiance[i].append_derivative_dependence(temperature, &dr_dt[i], n, 2);
   }
 }
\end{lstlisting}
%
In this example, the form of \code{add\_derivative\_dependence} for
one variable on the right-hand-side of the derivative expression has
been used, and the form of \code{append\_derivative\_dependence} for
an array of variables on the right-hand-side has been used. As
described in section \ref{sec:adouble}, both functions have forms that
take single variables and arrays as arguments. Note also that the use
of \code{std::vector<double>} rather than \code{new double[n]} ensures
that if \code{simulate\_radiances} throws an exception, the memory
allocated to hold \code{dr\_dt} will be freed correctly.

\section{Parallelizing \Adept\ programs}
\Adept\ currently has limited built-in support for parallelization. If
the algorithms that you wish to differentiate are individually small
enough to be treated by a single processor core, and you wish to
differentiate multiple algorithms independently (or the same algorithm
but with multiple sets of inputs) then parallelization is
straightforward. This is because the global variable containing a
pointer to the \Adept\ stack uses thread-local storage.  This means
that if a process spawns multiple threads (e.g.\ using OpenMP or
Pthreads) then each thread can declare one \code{adept::Stack} object
and all \code{adouble} operations will result in statements being
stored on the stack object specific to that thread.  The
\Adept\ package contains a test program \code{test\_thread\_safe} that
demonstrates this approach in OpenMP.

If your problem is larger and you wish to use parallelism to speed-up
the differentiation of a single large algorithm then the build-in
support is more limited. Provided your program and the \Adept\ library
were compiled with OpenMP enabled (which is the default for the
\Adept\ library if your compiler supports OpenMP), the computation of
Jacobian matrices will be parallelized.  By default, the maximum
number of concurrent threads will be equal to the number of available
cores, but this can be overridden with the
\code{set\_max\_jacobian\_threads} member function of the \code{Stack}
class.  Note that the opportunity for speed-up depends on the size of
your Jacobian matrix: for an $m\times n$ matrix, the number of
independent passes through the stored data is $\mathrm{min}(m,n)$ and
each thread treats \code{ADEPT\_MULTIPASS\_SIZE} of them (see section
\ref{sec:configuring_lib}), so the maximum number of threads that can
be exploited is $\mathrm{min}(m,n)/$\code{ADEPT\_MULTIPASS\_SIZE}.
Again, the \code{test\_thread\_safe} program can demonstrate the
parallelization of Jacobian calculations.  Note, however, that if the
\code{jacobian} function is called from within an OpenMP thread
(e.g.\ if the program already uses OpenMP with each thread containing
its own \code{adept::Stack} object), then the program is likely not to
be able to spawn more threads to assist with the Jacobian calculation.

If you need Jacobian matrices then the ability to parallelize the
calculation of them is useful since this tends to be more
computationally costly than recording the original algorithm.  If you
only require the tangent-linear or adjoint calculations (equivalent to
a Jacobian calculation with $n=1$ or $m=1$, respectively), then
unfortunately you are stuck with single threading. It is intended that
a future version of \Adept\ will enable all aspects of differentiating
an algorithm to be parallelized with either or both of OpenMP and MPI.

\section{Tips for the best performance}
\label{sec:tips}
\begin{itemize}
\item If you are working with single-threaded code, or in a
  multi-threaded program but with only one thread using a Stack
  object, then you can get slightly faster code by compiling all of
  your code with \code{-DADEPT\_STACK\_THREAD\_UNSAFE}. This uses a
  standard (i.e. non-thread-local) global variable to point to the
  currently active stack object, which is slightly faster to access.
\item If you compile with the \code{-g} option to store debugging
  symbols, your object files and executable will be much larger
  because every mathematical statement in the file will have the name
  of its associated templated type stored in the file, and these names
  can be long. Once you have debugged your code, you may wish to omit
  debugging symbols from production versions of the executable.  There
  appears to be no performance penalty associated with the debugging
  symbols, at least with the GNU C++ compiler.
\item A high compiler optimization setting is recommended to inline
  the function calls associated with mathematical expressions.  On the
  GNU C++ compiler, the \code{-O3} setting is recommended.
\item By default the Jacobian functions are compiled to process a
  strip of rows or columns of the Jacobian matrix at once. The optimum
  width of the strip depends on your platform, and you may wish to
  change it. To make the Jacobian functions process \textit{n} rows or
  columns at once, recompile the \Adept\ library with
  \code{-DADEPT\_MULTIPASS\_SIZE=}\textit{n}.
\item If you suspect memory usage is a problem, you may investigate
  the memory used by \Adept\ by simply sending your \code{Stack} object to a
  stream, e.g. ``\code{std::cout \textless\textless\ stack}''. You may
  also use the \code{memory()} member function, which returns the
  total number of bytes used. Further details of similar functions is
  given in section \ref{sec:stack}.
\end{itemize}

\section{Member functions of the \codestyle{Stack} class}
\label{sec:stack}
This section describes the user-oriented member functions of the
\code{Stack} class. Some functions have arguments with default values;
if these arguments are omitted then the default values will be used
(for example, if only one argument is supplied to the
\codebf{jacobian} function below, then it will be executed as if
called with a second argument \codebf{false}).  Some of these
functions throw \Adept\ exceptions, defined in section
\ref{sec:status}.

\begin{description}
\citem{Stack(bool activate\_immediately = true)} The constructor for the
\codebf{Stack} class.  Normally \codebf{Stack} objects are constructed
with no arguments, which means that the object will attempt to make
itself the currently active stack by placing a pointer to itself into
a global variable.  If another \codebf{Stack} object is currently
active, then the present one will be fully constructed, left in the
unactivated state, and an \code{stack\_already\_active} exception
will be thrown.  If a \codebf{Stack} object is constructed with an
argument ``\codebf{false}'', it will be started in an unactivated
state, and a subsequent call to its member function \codebf{activate}
will be needed to use it.
%
\citem{void new\_recording()} Clears all the information on the stack
in order that a new recording can be started. Specifically this
function clears all the differential statements, the list of
independent and dependent variables (used in computing Jacobian
matrices) and the list of gradients used by the
\codebf{compute\_tangent\_linear} and \codebf{compute\_adjoint} functions.
Note that this function leaves the memory allocated to reduce the
overhead of reallocation in the new recordings.
%
\citem{bool pause\_recording()} Stops recording differential
  information every time an \code{adouble} statement is
  executed. This is useful if within a single program an algorithm
  needs to be run both with and without automatic
  differentiation. This option is only effective within compilation
  units compiled with \code{ADEPT\_RECORDING\_PAUSABLE} defined; if it is,
  the function returns \code{true}, otherwise it returns
  \code{false}. Further information on using this and the following
  function are provided in section \ref{sec:pausable}.
%
\citem{bool continue\_recording()} Instruct a stack that may have
previously been put in a paused state to now continue recording
differential information as normal.  This option is only effective within
compilation units compiled with \code{ADEPT\_RECORDING\_PAUSABLE}
defined; if it is, the function returns \code{true}, otherwise it
returns \code{false}.
%
\citem{bool is\_recording()} Returns \code{false} if recording has
  been paused with \code{pause\_recording()} and the code has been
  compiled with \code{ADEPT\_RECORDING\_PAUSABLE} defined.
  Otherwise returns \code{true}.
%
\citem{void compute\_tangent\_linear()} Perform a tangent-linear
calculation (forward-mode differentiation) using the stored
differential statements.  Before calling this function you need call
the \code{adouble::set\_gradient} or \code{set\_gradients} function (see
section \ref{sec:adouble}) on the independent variables to set the
initial gradients, otherwise the function will throw a
\code{gradients\_not\_initialized} exception. This function is
synonymous with \codebf{forward()}.
%
\citem{void compute\_adjoint()} Perform an adjoint calculation
(reverse-mode differentiation) using the stored differential
statements.  Before calling this function you need call the
\code{adouble::set\_gradient} or \code{set\_gradients} function on the
dependent variables to set the initial gradients, otherwise the
function will throw a \code{gradients\_not\_initialized}
exception. This function is synonymous with \codebf{reverse()}.
%
\citem{void independent(const adouble\&\ x)} Before computing Jacobian
  matrices, you need to identify the independent and dependent
  variables, which correspond to the columns and rows of he Jacobian,
  respectively. This function adds \codebf{x} to the list of
  independent variables. If it is the $n$th variable identified in
  this way, the $n$th column of the Jacobian will correspond to
  derivatives with respect to \codebf{x}.
\citem{void dependent(const adouble\&\ y)} Add \codebf{y} to the
  list of dependent variables.  If it is the $m$th variable identified
  in this way, the $m$th row of the Jacobian will correspond to
  derivatives of \codebf{y} with respect to each of the independent
  variables.
\citem{void independent(const adouble* x\_ptr, \Offset\ n)} Add
  \codebf{n} independent variables to the list, which must be
  stored consecutively in memory starting at the memory pointed to by
  \codebf{x\_ptr}.
\citem{void dependent(const adouble* y\_ptr, \Offset\ n)} Add
\codebf{n} dependent variables to the list, which must be stored
consecutively in memory starting at the memory pointed to by
\codebf{y\_ptr}.
%
\citem{void jacobian(double* jacobian\_out)} Compute the Jacobian matrix, i.e., the gradient of the $m$
dependent variables (identified with the \codebf{dependent(...)}
function) with respect to the $n$ independent variables (identified
with \codebf{independent(...)}. The result is returned in the memory
pointed to by \codebf{jacobian\_out}, which must have been allocated
to hold $m\times n$ values. The result is stored in
column-major order, i.e., the $m$ diemension of the matrix varies
fastest. If no dependents or independents have been identified,
then the function will throw a
\code{dependents\_or\_independents\_not\_identified} exception. In
practice, this function calls \codebf{jacobian\_forward} if $n\le
m$ and \codebf{jacobian\_reverse} if $n>m$.
%
\citem{void jacobian\_forward(double* jacobian\_out)} Compute the Jacobian matrix by executing
$n$ forward passes through the stored list of differential statements;
this is typically faster than \codebf{jacobian\_reverse} for $n\le
m$.
%
\citem{void jacobian\_reverse(double* jacobian\_out)} Compute the Jacobian matrix by executing
$m$ reverse passes through the stored list of differential statements;
this is typically faster than \codebf{jacobian\_forward} for
$n>m$.
%
\citem{void clear\_gradients()} Clear the gradients set with the
\code{set\_gradient} member function of the \code{adouble} class. This
enables multiple adjoint and/or tangent-linear calculations to be
performed with the same recording.
%
\citem{void clear\_independents()} Clear the list of independent
variables, enabling a new Jacobian matrix to be computed from the same
recording but for a different set of independent variables.
%
\citem{void clear\_dependents()} Clear the list of dependent
variables, enabling a new Jacobian matrix to be computed from the same
recording but for a different set of dependent variables.
%
\citem{\Offset\ n\_independents()} Return the number of independent
variables that have been identified.
%
\citem{\Offset\ n\_dependents()} Return the number of dependent
variables that have been identified.
%
\citem{\Offset\ n\_statements()} Return the number of differential
statements in the recording.
%
\citem{\Offset\ n\_operations()} Return the total number of operations
in the recording, i.e the total number of terms on the right-hand-side
of all the differential statements.
%
\citem{\Offset\ max\_gradients()} Return the number of working gradients
that need to be stored in order to perform a forward or reverse pass.
%
\citem{std::size\_t memory()} Return the number of bytes currently
used to store the differential statements and the working
gradients. Note that this does not include memory allocated but not
currently used.
%
\citem{\Offset\ n\_gradients\_registered()} Each time an
\code{adouble} object is created, it is allocated a unique index that
is used to identify its gradient in the recorded differential
statements. When the object is destructed, its index is freed for
reuse. This function returns the number of gradients currently
registered, equal to the number of \code{adouble} objects currently
created.
%
\citem{void print\_status(std::ostream\&\ os = std::cout)} Print the
current status of the \codebf{Stack} object, such as number of
statements and operations stored and allocated, to the stream
specified by \codebf{os}, or standard output if this function is
called with no arguments.  Sending the \codebf{Stack} object to the
stream using the ``\code{<<}'' operator results in the same behaviour.
%
\citem{void print\_statements(std::ostream\&\ os = std::cout)} Print
the list of differential statements to the specified stream (or
standard output if not specified). Each line corresponds to a separate
statement, for example ``\code{d[3] = 1.2*d[1] + 3.4*d[2]}''.
%
\citem{bool print\_gradients(std::ostream\&\ os = std::cout)} Print
the vector of gradients to the specified stream (or standard output if
not specified). This function returns
\code{false} if no \code{set\_gradient}
function has been called to set the first gradient and initialize the
vector, and \code{true} otherwise. To diagnose what
\codebf{compute\_tangent\_linear} and 
\codebf{compute\_adjoint} are doing, it can be useful to call
\codebf{print\_gradients} immediately before and after.
%
\citem{void activate()} Activate the \codebf{Stack} object by copying
its \code{this} pointer to a global variable that will be accessed by
subsequent operations involving \code{adouble} objects.  If another
\codebf{Stack} is already active, a \code{stack\_already\_active}
exception will be thrown. To check whether this is the case before
calling \codebf{activate()}, check that the \code{active\_stack()}
function (described below) returns \code{0}.
%
\citem{void deactivate()} Deactivate the \codebf{Stack} object by
checking whether the global variable holding the pointer to the
currently active \codebf{Stack} is equal to \code{this}, and if it is,
setting it to \code{0}.
%
\citem{bool is\_active()} Returns \code{true} if the \codebf{Stack}
object is the currently active one, \code{false} otherwise.
%
\citem{void start()} This function was present in version 0.9 to
activate a \codebf{Stack} object, since in that version they were not
constructed in an activated state.  This function has now been
deprecated and will always throw a \code{feature\_not\_available}
exception.
\citem{int max\_jacobian\_threads()} Return the maximum number of
OpenMP threads available for Jacobian calculations.  The number will
be 1 if either the library was or the current source code is compiled
without OpenMP support (i.e.\ without the \code{-fopenmp} compiler and
linker flag). (Introduced in \Adept\ version 1.1.) 
\citem{int set\_max\_jacobian\_threads(int n)} Set the maximum number of
threads to be used in Jacobian calculations to \code{n}, if
possible. A value of 1 indicates that OpenMP will not be used, while a
value of 0 indicates that the maximum available will be used. Returns
the maximum that will be used, which may be fewer than requested,
e.g. 1 if the \Adept\ library was compiled without OpenMP
support. (Introduced in \Adept\ version 1.1.) 
\end{description}

\noindent The following non-member functions are provided in the
\code{adept} namespace:
\begin{description}
\citem{adept::Stack* active\_stack()} Returns a pointer to the
currently active \codebf{Stack} object, or \code{0} if there is none.
\citem{bool is\_thread\_unsafe()} Returns \code{true} if your code has
been compiled with \code{ADEPT\_STACK\_THREAD\_UNSAFE}, \code{false}
otherwise.
\citem{std::string version()} Returns a string containing the version
  number of the \Adept\ library (e.g. ``\code{1.1}'').
\citem{std::string compiler\_version()} Returns a string containing
the compiler name and version used to compile the \Adept\ library.
\citem{std::string compiler\_flags()} Returns a string containing the
compiler flags used when compiling the \Adept\ library.
%
\end{description}


\section{Member functions of the \codestyle{adouble} object}
\label{sec:adouble}
This section describes the user-oriented member functions of the
\code{adouble} class. Some functions have arguments with default
values; if these arguments are omitted then the default values will be
used. Some of these functions throw \Adept\ exceptions, defined in
section \ref{sec:status}.
\begin{description}
\citem{double value()} Return the underlying \code{double} value.
%
\citem{void set\_value(double x)} Set the value of the \codebf{adouble}
object to \codebf{x}, without storing the equivalent differential
statement in the currently active stack.
%
\citem{void set\_gradient(const double\&\ gradient)} Set the
gradient corresponding to this \codebf{adouble} variable. The first call
of this function (for any \codebf{adouble} variable) after a new
recording is made also initializes the vector of working gradients.
This function should be called for one or more \codebf{adouble} objects
after a recording has been made but before a call to
\code{Stack::compute\_tangent\_linear()} or
\code{Stack::compute\_adjoint()}.
%
\citem{void get\_gradient(double\&\ gradient)} Set \codebf{gradient}
to the value of the gradient corresponding to this \codebf{adouble}
object. This function is used to extract the result after a call to
\code{Stack::compute\_tangent\_linear()} or
\code{Stack::compute\_adjoint()}. If the \codebf{set\_gradient} function
was not called since the last recording was made, this function will
throw a \code{gradients\_not\_initialized} exception.  The function
can also throw a \code{gradient\_out\_of\_range} exception if new
\codebf{adouble} objects were created since the first
\codebf{set\_gradient} function was called.
%
\citem{void add\_derivative\_dependence(const adouble\&\ r, const
  adouble\&\ g)} Add a differential statement to the currently active
stack of the form $\delta \codebf{l}=\codebf{g}\times\delta
\codebf{r}$, where \codebf{l} is the \codebf{adouble} object from which
this function is called.  This function is needed to interface to
software containing hand-coded Jacobians, as described in section
\ref{sec:interfacehandcoded}; in this case \codebf{g} is the gradient
$\partial\codebf{l}/\partial\codebf{r}$ obtained from such software.
%
\citem{void append\_derivative\_dependence(const adouble\&\ r, const
  adouble\&\ g)} Assuming that the same \codebf{adouble} object has just
had its \codebf{add\_derivative\_dependence} member function called,
this function appends ${}+\codebf{g}\times\delta\codebf{r}$ to the
most recent differential statement on the stack.  If the calling
\codebf{adouble} object is different, then a \code{wrong\_gradient}
exception will be thrown. Note that multiple
\codebf{append\_derivative\_dependence} calls can be made in succession.
%
\item[\begin{minipage}{\textwidth}\codesize\texttt{void 
add\_derivative\_dependence(const adouble* r, const double* g,}\\ 
\mbox{ }\texttt{\hspace{18em}\Offset\ n = 1, \Offset\
      m\_stride = 1)}\end{minipage}]
%
Add a differential statement to the currently active stack of the form
$\delta\codebf{l}=\sum_{i=0}^{\codebf{n}-1}\codebf{m[}i\codebf{]}
\times\delta\codebf{r[}i\codebf{]}$, where \codebf{l} is the \codebf{adouble}
object from which this function is called. If the \codebf{g\_stride}
argument is provided, then the index to the \codebf{g} array will be
$i\times\codebf{g\_stride}$ rather than $i$.  This is useful if the
Jacobian provided is oriented such that the relevant gradients for
\codebf{l} are not spaced consecutively.
%
\item[\begin{minipage}{\textwidth}\codesize\texttt{void 
append\_derivative\_dependence(const adouble* rhs, const double* g,}\\ 
\mbox{ }\texttt{\hspace{20em}\Offset\ n = 1, \Offset\
      g\_stride = 1)}\end{minipage}]
%
Assuming that the same \codebf{adouble} object has just called the
\codebf{add\_derivative\_dependence} function, this function appends
${}+\sum_{i=0}^{\codebf{n}-1}\codebf{m[}i\codebf{]}
\times\delta\codebf{r[}i\codebf{]}$ to the most recent differential
statement on the stack. If the calling \codebf{adouble} object is
different, then a \code{wrong\_gradient} exception will be
thrown. The \codebf{g\_stride} argument behaves the same way as in the
previous function described.
\end{description}

\noindent The following non-member functions are provided in the
\code{adept} namespace:
\begin{description}
\citem{double value(const adouble\& x)} Returns the underlying
value of \codebf{x} as a \codebf{double}. This is useful to enable
\codebf{x} to be used in \code{fprintf} function calls. It is
generally better to use \codebf{adept::value(x)} rather than
\codebf{x.value()}, because the former also works if you compile the
code with the \code{ADEPT\_NO\_AUTOMATIC\_DIFFERENTIATION} flag set,
as discussed in section \ref{sec:multipleobjects}.
%
\citem{void set\_values(adouble* x, \Offset\ n, const double* x\_val)}
Set the value of the \codebf{n} \codebf{adouble} objects starting at
\codebf{x} to the values in \codebf{x\_val}, without storing the
equivalent differential statement in the currently active stack.
%
\citem{void set\_gradients(adouble* x, size\_t n, const double*
  gradients)} Set the gradients corresponding to the \codebf{n}
\codebf{adouble} objects starting at \codebf{x} to the \codebf{n}
\code{double}s starting at \codebf{gradients}.  This has the same
effect as calling the \codebf{set\_gradient} member function of each
\codebf{adouble} object in turn, but is more concise.
%
\citem{void get\_gradients(const adouble* y, size\_t n, double*
  gradients)} Copy the gradient of the \codebf{n} \codebf{adouble}
objects starting at \codebf{y} into the \codebf{n} \code{double}s
starting at \codebf{gradients}. This has the same effect as calling
the \codebf{get\_gradient} member function of each \codebf{adouble} object
in turn, but is more concise.  This function can throw a
\code{gradient\_out\_of\_range} exception if new \codebf{adouble}
objects were created since the first \codebf{set\_gradients} function
or \codebf{set\_gradient} member function was called.
\end{description}

\section{Exceptions thrown by the \Adept\ library}
\label{sec:status}
Some functions in the \Adept\ library can throw exceptions, and all of
the exceptions that can be thrown are derived from
\code{adept::autodiff\_exception}, which is itself derived from
\code{std::exception}. All these exceptions indicate an error in the
users code, usually associated with calling \Adept\ functions in the
wrong order.

An exception-catching implementation that takes different actions
depending on whether a specific \Adept\ exception, a general \Adept\
exception, or a non-\Adept\ exception is thrown might have the following
form:
%
\begin{lstlisting}
 try {
   adept::Stack stack;
   // ... Code using the Adept library goes here ...
 }
 catch (adept::stack_already_active& e) {
   // Catch a specific Adept exception
   std::cerr << "Error: " << e.what() << std::endl;
   // ... any further actions go here ...
 }
 catch (adept::autodiff_exception& e) {
   // Catch any Adept exception not yet caught
   std::cerr << "Error: " << e.what() << std::endl;
   // ... any further actions go here ...
 }
 catch (...) {
   // Catch any exceptions not yet caught
   std::cerr << "An error occurred" << std::endl;
   // ... any further actions go here ...
 }
\end{lstlisting}
%
All exceptions implement the \code{what()} member function, which
returns a \code{const char*} containing an error message. The
following exceptions can be thrown, and all are in the \code{adept}
namespace:
\begin{description}
\citem{gradient\_out\_of\_range} This exception can be thrown by the
\code{adouble::get\_gradient} member function if the index to its
gradient is larger than the number of gradients stored.  This can
happen if the \code{adouble} object was created after the first
\code{adouble::set\_gradient} call since the last
\code{Stack::new\_recording} call. The first
\code{adouble::set\_gradient} call signals to the \Adept\ stack that
the main algorithm has completed and so memory can be allocated to
store the gradients ready for a forward or reverse pass through the
differential statements. If further \code{adouble} objects are created
then they may have a gradient index that is out of range of the memory
allocated.
%
\citem{gradients\_not\_initialized} This exception can be thrown by
functions that require the list of working gradients to have been
initialized (particularly the functions
\code{Stack::compute\_tangent\_linear} and
\code{Stack::compute\_adjoint}). This initialization occurs when
\code{adouble::set\_gradient} is called.
%
\citem{stack\_already\_active} This exception is thrown when an
attempt is made to make a particular \code{Stack} object ``active'',
but there already is an active stack in this thread. This can be
thrown by the \code{Stack} constructor or the \code{Stack::activate}
member function.
%
\citem{dependents\_or\_independents\_not\_identified} This exception
is thrown when an attempt is made to compute a Jacobian but the
independents and/or dependents have not been identified.
%
\citem{wrong\_gradient} This exception is thrown by the
\code{adouble::append\_derivative\_dependence} if the \code{adouble}
object that it is called from is not the same as that of the most
recent \code{adouble::add\_derivative\_dependence}. 
%
\citem{non\_finite\_gradient} This exception is thrown if the users
code is compiled with the preprocessor variable
\code{ADEPT\_TRACK\_NON\_FINITE\_GRADIENTS} defined, and a
mathematical operation is carried out for which the derivative is not
finite. This is useful to locate the source of non-finite derivatives
coming out of an algorithm.
%
\citem{feature\_not\_available} This exception is thrown by deprecated
functions, such as \code{Stack::start()}.
\end{description}

\section{Configuring the behaviour of \Adept}
\label{sec:configuring}
The behaviour of the \Adept\ library can be changed by defining one or
more of the \Adept\ preprocessor variables. This can be done either by
editing the \code{adept.h} file and uncommenting the relevant
\code{\#define} lines in sections 1 or 2 of the file, or by compiling
your code with \code{-Dxxx} compiler options (replacing \code{xxx} by
the relevant preprocessor variable. There are two types of
preprocessor variable: the first types only apply to the compilation
of user code, while the second types require the \Adept\ library to be
recompiled.

\subsection{Modifications not requiring a library recompile}
\label{sec:configuring_no_lib}
The preprocessor variables that apply only to user code and do not
require the \Adept\ library to be recompiled are as follows:
\begin{description}
\citem{ADEPT\_STACK\_THREAD\_UNSAFE} If this variable is defined, the
currently active stack is stored as a global variable but is not
defined to be ``thread-local''. This is slightly faster, but means
that you cannot use multi-threaded code with separate threads holding
their own active \code{Stack} object. Note that although defining this
variable does not require a library recompile, all source files that
make up a single executable must be compiled with this option (or all
not be).
%
\citem{ADEPT\_RECORDING\_PAUSABLE} This option enables an algorithm to
be run both with and without automatic differentiation from within the
same program via the functions \code{Stack::pause\_recording()} and
\code{Stack::continue\_recording()}.  Note that although defining this
variable does not require a library recompile, all source files that
make up a single executable must be compiled with this option (or all
not be). Further details on this option are provided in section
\ref{sec:pausable}.
%
\citem{ADEPT\_NO\_AUTOMATIC\_DIFFERENTIATION} This option turns off
automatic differentiation by treating \code{adouble} objects as
\code{double}. It is useful if you want to compile one source file
twice to produce versions with and without automatic
differentiation. Further details on this option are provided in
section \ref{sec:multipleobjects}.
%
\citem{ADEPT\_TRACK\_NON\_FINITE\_GRADIENTS} Often when an algorithm
is first converted to use an operator-overloading automatic
differentiation library, the gradients come out as Not-a-Number or
Infinity. The reason is often that the algorithm contains operations
for which the derivative is not finite (e.g.\ $\sqrt{a}$ for $a=0$),
or constructions where a non-finite value is produced but subsequently
made finite (e.g.\ $\exp(-1.0/a)$ for $a=0$). Usually the algorithm
can be recoded to avoid these problems, if the location of the
problematic operations can be identified. By defining this
preprocessor variable, a \code{non\_finite\_gradient} exception will
be thrown if any operation results in a non-finite derivative. Running
the program within a debugger (and ensuring that the exception is not
caught within the program) enables the offending line to be
identified.
%
\citem{ADEPT\_INITIAL\_STACK\_LENGTH} This preprocessor variable is
set to an integer, and is used as the default initial amount of memory
allocated for the recording, in terms of the number of statements and
operations.
%
\citem{ADEPT\_REMOVE\_NULL\_STATEMENTS} If many variables in your code
are likely to be zero then redundant operations will be added to the
list of differential statements. For example, the assignment
$a=b\times c$ with active variables $b$ and $c$ both being zero
results in the differential statement $\delta a=0\times\delta
b+0\times\delta c$. This preprocessor variable checks for zeros and
removes terms on the right-hand-side of differential statements if it
finds them. In this case it would put $\delta a=0$ on the stack
instead. This option slows down the recording stage, but speeds up the
subsequent use of the recorded stack for adjoint and Jacobian
calculations. The speed up of the latter is only likely to exceed the
slow down of the former if your code contains many zeros. For most
codes, this option causes a net slow down.
%
\citem{ADEPT\_COPY\_CONSTRUCTOR\_ONLY\_ON\_RETURN\_FROM\_FUNCTION} If
copy constructors for \code{adouble} objects are only used in the
return values for functions, then defining this preprocessor variable
will lead to slightly faster code, because it will be assumed that
when a copy constructor is called, the index to its gradient can
simply be copied because the object being copied will shortly be
destructed (otherwise communication with the \code{Stack} object is
required to unregister one and immediately register the other). You
need to be sure that the code being compiled with this option does not
invoke the copy constructor in any other circumstances. Specifically,
it must not include either of these constructions: ``\code{adouble x =
  y;}'' or ``\code{adouble x(y);}'', where \code{y} is an
  \code{adouble} object. If it does, then strange errors will occur.
\end{description}

\subsection{Modifications requiring a library recompile}
\label{sec:configuring_lib}
\noindent The preprocessor variables that require the \Adept\ library
to be recompiled are as follows. Note that if these variables are used
they must be the same when compiling both the library and the user
code. This is safest to implement by editing section 2 of the
\code{adept.h} header file.
\begin{description}
\citem{ADEPT\_FLOATING\_POINT\_TYPE} If you want to compile \Adept\ to
use a precision other than double, define this preprocessor variable
to be the floating-point type required, e.g. \code{float} or
\code{long double}. To use from the compiler command-line, use the
argument \code{-DADEPT\_FLOATING\_POINT\_TYPE=float} or
\code{-DADEPT\_FLOATING\_POINT\_TYPE="long double"}.
%
\citem{ADEPT\_STACK\_STORAGE\_STL} Use the C++ standard template
library \code{vector} or \code{valarray} classes for storing the
recording and the list of gradients, rather than dynamically allocated
arrays. In practice, this tends to slow down the code.
%
\citem{ADEPT\_MULTIPASS\_SIZE} This is set to an integer, invariably a
power of two, specifying the number of rows or columns of a Jacobian
that are calculated at once. The optimum value depends on the platform
and the capability of the compiler to optimize loops whose length is
known at compile time.
% 
\citem{ADEPT\_MULTIPASS\_SIZE\_ZERO\_CHECK} This is also set to an
integer; if it is greater than \codebf{ADEPT\_MULTIPASS\_SIZE}, then
the \code{Stack::jacobian\_reverse} function checks gradients are
non-zero before using them in a multiplication.
%
\citem{ADEPT\_THREAD\_LOCAL} This can be used to specify the way that
thread-local storage is declared by your compiler.  Thread-local
storage is used to ensure that the \Adept\ library is thread-safe. By
default this variable is not defined initially, and then later in
\code{adept.h} it is set to \code{\_\_declspec(thread)} on Microsoft
Visual C++, an empty declaration on Mac (since thread-local storage is
not available on many Mac platforms) and \code{\_\_thread} otherwise
(appropriate for at least the GCC, Intel, Sun and IBM compilers). To
override the default behaviour, define this variable yourself in
section 2 of \code{adept.h}.
\end{description}

\section{Frequently asked questions}
\label{sec:faq}
\begin{description}
\item[Why are all the gradients coming out of the automatic
  differentiation zero?] You have almost certainly omitted or
  misplaced the call of the \code{adept::Stack} member function
  ``\code{new\_recording()}''. It should be placed \emph{after} the
  independent variables in the algorithm have been initialized, but
  before any subsequent calculations are performed on these
  variables. If it is omitted or placed before the point where the
  independent variables are initialized, the differential statements
  corresponding to this initialization (which are all of the form
  $\delta x=0$), will be placed in the list of differential statements
  and will unhelpfully set to zero all your gradients right at the
  start of a forward pass (resulting from a call to \code{forward()})
  or set them to zero right at the end of a reverse pass (resulting
  from a call to \code{reverse()}).
\item[Why are the gradients coming out of the automatic
  differentiation NaN or Inf (even though the value is correct)?] This
  can occur if the algorithm contains operations for which the
  derivative is not finite (e.g.\ $\sqrt{a}$ for $a=0$), or
  constructions where a non-finite value is produced but subsequently
  made finite (e.g.\ $\exp(-1.0/a)$ for $a=0$). Usually the algorithm
  can be recoded to avoid these problems, if the location of the
  problematic operations can be identified. The simplest way to locate
  the offending statement is to recompile your code with the \code{-g}
  option and the \code{ADEPT\_TRACK\_NON\_FINITE\_GRADIENTS}
  preprocessor variable set (see section
  \ref{sec:configuring_no_lib}). Run the program within a debugger and
  a \code{non\_finite\_gradient} exception will be thrown, which if
  not caught within the program will enable you to locate the line in
  your code where the problem originated.  You may need to turn
  optimizations off (compile with \code{-O0}) for the line
  identification to be accurate. Another approach is to add the
  following in a C++ source file:
\begin{lstlisting}
#include <fenv.h>
int _feenableexcept_status = feenableexcept(FE_INVALID|FE_DIVBYZERO|FE_OVERFLOW);
\end{lstlisting}
  This will cause a floating point exception to be thrown when a
  \code{NaN} or \code{Inf} is generated, which can again be located in
  a debugger.
\item[Why are the gradients coming out of the automatic
  differentiation wrong?] Before suspecting a bug in \Adept, note that
  round-off error can lead to incorrect gradients even in hand-coded
  differential code. Consider the following:
\begin{lstlisting}
int main() {
  Stack stack;
  adouble a = 1.0e-26, b;
  stack.new_recording();
  b = sin(a) / a;
  b.set_gradient(1.0);
  stack.compute_adjoint();
  std::cout << "a=" << a << ", b=" << b << ", db/da=" << a.get_gradient() << "\n";
}
\end{lstlisting}
  We know that near \code{a=0} we should have \code{b=1} and the
  gradient should be \code{0}.  But running the program above will
  give a gradient of \code{1.71799e+10}. If you hand-code the
  gradient, i.e.
\begin{lstlisting}
double A = 1.0e-26;
double dB_dA = cos(A)/A - sin(A) / (A*A);
\end{lstlisting}
  you will you will also get the wrong gradient.  You can see that the
  answer is the difference of two very large numbers and so subject to
  round-off error.  This example is therefore not a bug of \Adept, but
  a limitation of numerical gradients.  To check this, try compiling
  your code using either the ADOL-C or CppAD automatic differentiation
  tools; I have always found these tools to give exactly the same
  gradient as \Adept. Unfortunately, round-off error can build up over
  many operations to give the wrong result, so there may not be a
  simple solution in your case.
\item[Can \Adept\ reuse a stored tape for multiple runs of the same
  algorithm but with different inputs?] No. \Adept\ does not store the
  full algorithm in its stack (as ADOL-C does in its tapes, for
  example), only the derivative information.  So from the stack alone
  you cannot rerun the function with different inputs.  However,
  rerunning the algorithm including recomputing the derivative
  information is fast using \Adept, and is still faster than libraries
  that store enough information in their tapes to enable a tape to be
  reused with different inputs.  It should be stressed that for any
  algorithm that includes different paths of execution (``if''
  statements) based on the values of the inputs, such a tape would
  need to be rerecorded anyway. This includes any algorithm containing
  a look-up table.
\item[Why does my code crash with a segmentation fault?] This means it
  is trying to access a memory address not belonging to your program,
  and the first thing to do is to run your program in a debugger to
  find out at what point in your code this occurs. If it is in the
  \code{adept::aReal} constructor (note that \code{aReal} is synonymous with
  \code{adouble}), then it is very likely that you have tried to
  initiate an \code{adept::adouble} object before initiating an
  \code{adept::Stack} object. As described in section
  \ref{sec:stack_setup}, there are good reasons why you need to
  initialize the \code{adept::Stack} object first.
\item[How can I interface \Adept\ with a matrix library such as
  Eigen?]  Unfortunately the use of expression templates in
  \Adept\ means that it does not work optimally (if it works at all)
  with matrix libraries that use expression templates. However, I am
  working on a combined library that would combine array functionality
  with automatic differentiation, which is available as version 2.0 of
  \Adept.
\item[Do you have plans to enable \Adept\ to produce Hessian
  matrices?]  Not in the near future; adding array functionality is a
  higher priority at the moment.  However, if your objective function
  $J(\x)$ (also known as a cost function or penalty function) has a
  quadratic dependence on each of the elements of $\y(\x)$, where $\y$
  is a nonlinear function of the independent variables $\x$, then the
  Hessian matrix $\nabla_\x^2 J$ can be computed from the
  Jacobian matrix ${\bf H}=\partial\y/\partial\x$. This is the essence
  of the Gauss-Newton and Levenberg-Marquardt algorithms. Consider the
  optimization problem of finding the parameters $\x$ of nonlinear
  model $\y(\x)$ that provides the closest match to a set of
  ``observations'' $\y^o$ in a least-squares sense.  For maximum
  generality we add constraints that penalize differences between $\x$
  and a set of \emph{a~priori} values $\x^a$, as well as a
  regularization term.  In this case the objective function could be
  written as
\def\myspace{~~}
\begin{equation}
J(\x) \myspace =\myspace \left[\y(\x)-\y^o\right]^\mathrm{T}{\bf
  R}^{-1}\left[\y(\x)-\y^o\right]
\myspace+\myspace\left[\x-\x^a\right]^\mathrm{T}{\bf
  B}^{-1}\left[\x-\x^a\right]
\myspace+\myspace\x^\mathrm{T}{\bf T}\x.\nonumber
\label{eq:objective}
\end{equation}
  Here, all vectors are treated as column vectors, ${\bf R}$ is the
  error covariance matrix of the observations, ${\bf B}$ is the error
  covariance matrix of the \emph{a~priori} values, and ${\bf T}$ is a
  Twomey-Tikhonov matrix that penalizes either spatial gradients or
  curvature in $\x$.  The Hessian matrix is then given by
\begin{equation}
\nabla_\x^2J \myspace=\myspace {\bf H}^\mathrm{T}{\bf
  R}^{-1}{\bf H}\nonumber
\myspace+\myspace {\bf B}^{-1} \myspace+\myspace {\bf T},
\label{eq:hessian}
\end{equation}
  which can be coded up using \Adept\ to compute ${\bf H}$. Each term
  on the right-hand-side of (\ref{eq:hessian}) has its corresponding
  term in (\ref{eq:objective}), so it is easy to work out what the
  Hessian would look like if only a subset of the terms in
  (\ref{eq:objective}) were present.
\item[Why doesn't the ternary operator work?] Some compilers will fail
  to compile the following function:
\begin{lstlisting}
adept::adouble piecewise(adept::adouble x) {
  return x < 1.0 ? x*x : 2.0*x-1.0;
}
\end{lstlisting}%
The reason is that these compilers require that the two possible
outcomes of the ternary operator have the same type, but due to the
use of expression templates, the types of these mathematical
expressions are actually different.  The ternary operator cannot be
overloaded to allow such arguments. The solution is to explicitly
convert the outcomes to \code{adouble}:
\begin{lstlisting}
adept::adouble piecewise(adept::adouble x) {
  return x < 1.0 ? adept::adouble(x*x) : adept::adouble(2.0*x-1.0);
}
\end{lstlisting}
\item[Why is my executable so huge?]  Probably you are including
  debugging symbols by compiling with the \code{-g} option. Expression
  templates need long strings to describe them, so this extra content
  can increase the size of object files and executables by a factor of
  ten.  This does not slow down execution, but for production code you
  may wish to compile without debugging symbols.
\end{description}

\section{License for \Adept\ software}
\label{sec:license}

\def\thefootnote{\fnsymbol{footnote}} Version 1.1 of the
\Adept\ library is released under the Apache License, Version 2.0,
which is available at
\url{http://www.apache.org/licenses/LICENSE-2.0}.  In short, this
free-software license permits you to use the library for any purpose,
and to modify it and combine it with other software to form a larger
work.  If you choose, you may release the modified software in either
source code or object code form, so may use \Adept\ in both
open-source software and non-free proprietary software. However,
distributed versions must retain copyright notices and also distribute
both the information in the NOTICES file and a copy of the Apache
License.  Different license terms may be applied to your distributed
software, although they must include the conditions on redistribution
provided in the Apache License.  This is a short summary; if in doubt,
consult the text of the license.

In addition to the legally binding terms of the license, it is
\emph{requested} that:
\begin{itemize}
\item You cite \cite{Hogan2014} in publications describing algorithms
  and software that make use of the \Adept\ library. While not not a
  condition of the license, this is good honest practice in science
  and engineering.
\item If you make modifications to the \Adept\ library that might be
  useful to others, you release your modifications under the terms of
  the Apache License, Version 2.0, so that they are available to
  others and could also be merged into a future official version of
  \Adept.
\end{itemize}

Note that other source files in the \Adept\ package used for
demonstrating and benchmarking \Adept\ are released under the GNU
all-permissive license\footnote{The GNU all-permissive license reads:
  \emph{Copying and distribution of this file, with or without
    modification, are permitted in any medium without royalty provided
    the copyright notice and this notice are preserved.  This file is
    offered as-is, without any warranty.}}, which is specified at the
top of all files it applies to.

\Adept\ version 1.0 was released under the terms of the GNU General
Public License (GPL) and so could not be released as part of a larger
work unless the entire work was released under the conditions of the
GPL.  It is hoped that the switch to the Apache License will
facilitate wider use of \Adept.


\begin{thebibliography}{00}
\markright{References}
\harvarditem{Bell}{2007}{Bell2007}Bell, B., 2007: CppAD: A package for C++
algorithmic differentiation. \url{http://www.coin-or.org/CppAD}
% 
\harvarditem{Liu and Nocedal}{1989}{Liu+1989}Liu, D. C., and Nocedal,
  J., 1989: On the limited memory method for large scale
optimization. \emph{Math.\ Programming B,} {\bf 45,} 503--528.
%
\harvarditem{Gay}{2005}{Gay2005}Gay, D. M., 2005: Semiautomatic
differentiation for efficient gradient computations.  In
\emph{Automatic Differentiation: Applications, Theory, and
  Implementations}, H. M. B\"ucker, G. F. Corliss, P.  Hovland,
U. Naumann and B. Norris (eds.), Springer, 147--158.
%
\harvarditem{Griewank et~al.}{1996}{Griewank+1996}Griewank, A.,
  Juedes, D., and Utke, J., 1996:  Algorithm 755: ADOL-C: a package for the
automatic differentiation of algorithms written in C/C++. \textit{ACM
  Trans.\ Math.\ Softw.,} \textbf{22,} 131--167.
\harvarditem{Hogan}{2014}{Hogan2014}Hogan, R. J., 2014: Fast reverse-mode
  automatic differentiation using expression templates in
  C++. \textit{ACM Trans.\ Math.\ Softw.,} \textbf{40,} 26:1-26:16.
\end{thebibliography}

\end{document}
